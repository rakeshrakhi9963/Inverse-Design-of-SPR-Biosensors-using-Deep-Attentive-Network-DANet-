import pandas as pd
import numpy as np
import torch
from omegaconf import DictConfig

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error

from pytorch_tabular import TabularModel
from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig
from pytorch_tabular.models import DANetConfig

# ——— Fix for OmegaConf serialization in torch ———
torch.serialization.add_safe_globals({'omegaconf.dictconfig.DictConfig': DictConfig})

# ——— Load & Preprocess Data ———
df = pd.read_csv(r"C:\Users\madas\Downloads\spr_dataset_graphene_all_metals_sampled_80k.csv").dropna()

input_cols = ["FWHM_nm", "Sensitivity_nm_per_RIU", "NumA", "Sensing_length_mm", "n_sens", "Resonance_lambda_nm"]
output_cols = ["Silver_thickness_nm", "Gold_thickness_nm", "Copper_thickness_nm", "Aluminum_thickness_nm", "Graphene_layers"]

X = df[input_cols].values
y = df[output_cols].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_scaler = StandardScaler().fit(X_train)
y_scaler = StandardScaler().fit(y_train)

X_train = X_scaler.transform(X_train)
X_test = X_scaler.transform(X_test)
y_train = y_scaler.transform(y_train)
y_test = y_scaler.transform(y_test)

train_df = pd.DataFrame(X_train, columns=input_cols)
for i, col in enumerate(output_cols):
    train_df[col] = y_train[:, i]

test_df = pd.DataFrame(X_test, columns=input_cols)
for i, col in enumerate(output_cols):
    test_df[col] = y_test[:, i]

# ——— GPU-Aware Configs ———
data_config = DataConfig(
    target=output_cols,
    continuous_cols=input_cols,
)

model_config = DANetConfig(
    task="regression",
    n_layers=8,
    abstlay_dim_1=32,
    abstlay_dim_2=None,
    k=5,
    dropout_rate=0.1,
)

trainer_config = TrainerConfig(
    max_epochs=100,
    accelerator="gpu" if torch.cuda.is_available() else "cpu",
    devices=1,
    early_stopping=None,
)

optimizer_config = OptimizerConfig()

# ——— Initialize Model ———
tabular_model = TabularModel(
    data_config=data_config,
    model_config=model_config,
    trainer_config=trainer_config,
    optimizer_config=optimizer_config,
)

# ——— Train ———
tabular_model.fit(train=train_df, validation=test_df)

# ——— Predict & Inverse Transform ———
preds = tabular_model.predict(test_df)
y_pred = y_scaler.inverse_transform(preds["prediction"].values)
y_true = y_scaler.inverse_transform(y_test)

# ——— Evaluation ———
print("\n🔍 MAE for each output column:")
for i, col in enumerate(output_cols):
    mae = mean_absolute_error(y_true[:, i], y_pred[:, i])
    print(f"{col}: MAE = {mae:.4f}")
